% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/FinalProject11.R
\name{compute_metrics}
\alias{compute_metrics}
\title{Evaluate Binary Classification Model Performance}
\usage{
compute_metrics(predicted_probs, y, cutoff = 0.5)
}
\arguments{
\item{predicted_probs}{A \code{numeric} vector of predicted probabilities from the model.}

\item{y}{A \code{numeric} or \code{factor} vector of actual response values corresponding to the observations.
The values should be either 0 (negative class) or 1 (positive class)..}

\item{cutoff}{A \code{numeric} value indicating the threshold for classifying predictions as class 1.
The default cutoff is 0.5, meaning any predicted probability greater than 0.5 will be classified as 1.}
}
\value{
A \code{list} containing the following performance metrics:
\describe{
\item{Confusion_Matrix}{A \code{2x2} matrix showing the confusion matrix for the model,
with the predicted and actual values. It contains counts for true positives (TP), false positives (FP),
true negatives (TN), and false negatives (FN).}
\item{Prevalence}{The proportion of actual positives in the dataset.}
\item{Accuracy}{The proportion of correct predictions (TP + TN) out of all predictions.}
\item{Sensitivity}{The True Positive Rate (TPR), i.e., the proportion of actual positives correctly identified.}
\item{Specificity}{The True Negative Rate (TNR), i.e., the proportion of actual negatives correctly identified.}
\item{False_Discovery_Rate}{The proportion of false positives (FP) among all positive predictions.}
\item{Diagnostic_Odds_Ratio}{The odds ratio of the diagnostic test, calculated as (TP * TN) / (FP * FN).}
}
}
\description{
Computes various performance metrics to evaluate a binary classification model.
The function calculates metrics such as accuracy, sensitivity, specificity, and more,
based on predicted probabilities and the actual response values.
}
