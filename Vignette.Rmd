---
title: "Binary Optimization Vignette"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Final Project 11 Vignette

[<https://github.com/AU-R-Programming/FinalProject_11>]

This R package implements supervised binary classification using
numerical optimization. It includes functions to estimate regression
coefficients, compute bootstrap confidence intervals, and generate
confusion matrices with key metrics such as accuracy, sensitivity, and
specificity.

## Read in data

First we will need to install the package. This is done with the code
below.

```{r, warning=FALSE, results='hide', message=FALSE}

library(devtools)
install_github("AU-R-Programming/FinalProject_11")
library(FinalProject11)

```

## Preparing the Data

First we will need to prepare our data so that it is compatible with our
optimization functions. We will do this by reading in the csv and
choosing columns that will be compatible with binary classification. We
will also choose a predictor, y. For this example we will be using
`adult.csv` and looking at `education`, `relationship`, `race`, `sex` as
our predictors and we will look at the `NA` category as our response
(represents if the income is over/ under 50K). The function will handle
adding an intercept to our predictors, but we will need to ensure that
y, our response is a column of binary values.

```{r}
data <- read.csv("adult.csv", sep = ";")
chosen_columns <- c('education', 'relationship', 'race', 'sex')
X <- data[ ,chosen_columns]
# ensure y is a binary n x 1 matrix
y <- ifelse(data$NA. == " <=50K", 0, 1)
```

## Optimization

Now that our data is prepared we can pass these values into the
`omptimization_fn` function in our package. This function will estimate
the coefficient vector $\beta$ for our data. It will return a `beta_hat`
as well as an `init_beta`

```{r}
result <- optimization_fn(X, y)
beta_hat <- result$beta_hat
init_beta <- result$init_beta
```

Initial Beta below:

```{r}
init_beta
```

Estimated Beta below:

```{r}
beta_hat
```

## Confidence Interval

The `bootstrapCI` function computes bootstrap confidence intervals for
regression coefficients in a logistic regression model using resampling
with replacement. This method allows for the estimation of the
uncertainty of the regression coefficients by generating multiple
resamples of the data and calculating the coefficients for each
resample.

The function allows you to plug in:

-   X: A matrix or data frame containing the predictor variables. Each
    column represents a predictor, and the rows correspond to individual
    observations.

-   y: A vector containing the response values (dependent variable)
    corresponding to the observations in X.

-   alpha: A numeric value representing the significance level for the
    confidence intervals. The default is 0.05, which corresponds to a
    95% confidence interval.

-   B: The number of bootstrap resamples to perform. The default is 20.
    Increasing B can improve the precision of the confidence intervals
    but will require more computational time.

The confidence intervals for the `adult.csv` are:

```{r}
bootstrapCI(X, y, alpha = 0.05, B = 20)
```


## Confusion Matrix Metrics
```{r}
design <- model.matrix(~.,X)
predicted_probs <- get_predicted_prob(beta = beta_hat, X = design)
compute_metrics(predicted_probs = predicted_probs, y = y)

```

The process and steps to create the Confusion Matrix Metrics function are below:

Next we will create a confusion based off of the results from an if-else
statement, we assigned values for predictions based off of their cutoff
point. This cutoff point being 0.5, where values above are assigned 1,
and below the cutoff they are assigned 0. We will test the following
metrics on the confusion matrix made from the predictions. These are
prevalence, accuracy, sensitivity, specificity, false discovery rate,
and diagnostic odds ratio.

```{r}
#predicted_probs<-1/1+exp(-design %*% beta_optimized)
predictions<- ifelse(predicted_probs>0.5,1,0)
confusion_matrix<-table(Predicted=predicted_probs, Actual=y)

```

We will rename each quadrant of the confusion matrix for easier use when
computing our metrics.

```{r}
true_pos <- confusion_matrix[2, 2]  # Predicted = 1, Actual = 1
false_pos <- confusion_matrix[2, 1]  # Predicted = 1, Actual = 0
false_neg <- confusion_matrix[1, 2]  # Predicted = 0, Actual = 1
true_neg <- confusion_matrix[1, 1]   # Predicted = 0, Actual = 0
```

**Prevalence** This is the proportion of actual positives in the data
set out of all observations.

```{r}
prevalence <- (true_pos+false_neg)/sum(confusion_matrix)
```

**Accuracy** This is the proportion of corrected predictions for both
positive and negative out of all observations.

```{r}
accuracy <- (true_pos+true_neg)/sum(confusion_matrix)
```

**Sensitivity** Here we find the proportion of correctly predicted
positive instances out of all positive instances.

```{r}
sensitivity<-true_pos/(true_pos+false_neg)
```

**Specificity** Here this is the proportion of correctly predicted
negative instances out of all negative outcomes.

```{r}
specificity <- true_neg/(true_neg+false_pos)
```

**False Discovery Rate** This is the proportion of incorrect positive
predictions out of all positive outcomes.

```{r}
false_discovery_rate <- false_pos/(true_pos+false_pos)
```

**Diagnostic Odds Ratio** This metric summarizes the performances of
the diagnostic test by combining the sensitivity and specificity
metrics. High DOR indicates a good test with low false positives and
negatives.

```{r}
diagnostic_odds_ratio<-(true_pos+true_neg)/(false_pos+false_neg)
```
